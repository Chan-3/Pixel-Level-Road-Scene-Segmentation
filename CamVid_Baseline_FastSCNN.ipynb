{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92079aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu LR: 0.0003 EPOCHS: 60 BATCH: 4\n"
     ]
    }
   ],
   "source": [
    "# Stage 0 — Imports and global constants\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global configuration\n",
    "DATA_ROOT = \"./\"            # project root (AFML_project)\n",
    "CAMVID_ROOT = \"camvid\"\n",
    "CHECKPOINT_DIR = \"outputs/checkpoints\"\n",
    "OUTPUT_COMP = \"outputs/comparisons\"\n",
    "OUTPUT_DET = \"outputs/detection\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "LR = 3e-4\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 11\n",
    "IGNORE_INDEX = 255\n",
    "IMG_SIZE = 512\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_COMP, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DET, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Device:\", DEVICE, \"LR:\", LR, \"EPOCHS:\", EPOCHS, \"BATCH:\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc3672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted images zip\n",
      "Extracted labels zip\n",
      "Folder structure ready.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 — Create folder structure and optionally extract zips\n",
    "folders = [\n",
    "    f\"{CAMVID_ROOT}/images\",\n",
    "    f\"{CAMVID_ROOT}/labels\",\n",
    "    f\"{CAMVID_ROOT}/labels_processed\",\n",
    "    f\"{CAMVID_ROOT}/splits\",\n",
    "    f\"{CAMVID_ROOT}/mapping\",\n",
    "    f\"{CAMVID_ROOT}/raw_images\",\n",
    "    f\"{CAMVID_ROOT}/raw_labels\",\n",
    "]\n",
    "for f in folders:\n",
    "    os.makedirs(f, exist_ok=True)\n",
    "\n",
    "# If zip files exist in DATA_ROOT, extract to raw folders (safe: only if files exist)\n",
    "import zipfile\n",
    "zip_images = os.path.join(DATA_ROOT, \"701_StillsRaw_full.zip\")\n",
    "zip_labels = os.path.join(DATA_ROOT, \"LabeledApproved_full.zip\")\n",
    "\n",
    "if os.path.exists(zip_images) and not os.listdir(f\"{CAMVID_ROOT}/raw_images\"):\n",
    "    with zipfile.ZipFile(zip_images, \"r\") as z:\n",
    "        z.extractall(f\"{CAMVID_ROOT}/raw_images\")\n",
    "    print(\"Extracted images zip\")\n",
    "\n",
    "if os.path.exists(zip_labels) and not os.listdir(f\"{CAMVID_ROOT}/raw_labels\"):\n",
    "    with zipfile.ZipFile(zip_labels, \"r\") as z:\n",
    "        z.extractall(f\"{CAMVID_ROOT}/raw_labels\")\n",
    "    print(\"Extracted labels zip\")\n",
    "\n",
    "print(\"Folder structure ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ab3160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image and label files moved to camvid/images and camvid/labels\n"
     ]
    }
   ],
   "source": [
    "# Stage 2 — Move and rename files from raw dirs into camvid/images and camvid/labels\n",
    "\n",
    "def find_real_dir(path):\n",
    "    sub = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    return sub[0] if sub else path\n",
    "\n",
    "raw_img_dir = find_real_dir(f\"{CAMVID_ROOT}/raw_images\")\n",
    "raw_lbl_dir = find_real_dir(f\"{CAMVID_ROOT}/raw_labels\")\n",
    "\n",
    "img_dst = f\"{CAMVID_ROOT}/images\"\n",
    "lbl_dst = f\"{CAMVID_ROOT}/labels\"\n",
    "\n",
    "# Move labels (rename _L/_P)\n",
    "for fname in sorted(os.listdir(raw_lbl_dir)):\n",
    "    if fname.lower().endswith(\".png\"):\n",
    "        new = fname.replace(\"_L\", \"\").replace(\"_P\", \"\")\n",
    "        src = os.path.join(raw_lbl_dir, fname)\n",
    "        dst = os.path.join(lbl_dst, new)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Move images\n",
    "for fname in sorted(os.listdir(raw_img_dir)):\n",
    "    if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        src = os.path.join(raw_img_dir, fname)\n",
    "        dst = os.path.join(img_dst, fname)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "print(\"Image and label files moved to camvid/images and camvid/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97f9f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new train/val/test splits\n"
     ]
    }
   ],
   "source": [
    "# Stage 3 — Create train/val/test split files if not present\n",
    "img_dst = f\"{CAMVID_ROOT}/images\"\n",
    "split_dir = f\"{CAMVID_ROOT}/splits\"\n",
    "os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "all_files = sorted([f.replace(\".png\",\"\") for f in os.listdir(img_dst) if f.endswith(\".png\")])\n",
    "if not all_files:\n",
    "    raise ValueError(\"No .png images found in camvid/images — check path\")\n",
    "\n",
    "# Only create splits if not present\n",
    "if not (os.path.exists(f\"{split_dir}/train.txt\") and os.path.exists(f\"{split_dir}/val.txt\") and os.path.exists(f\"{split_dir}/test.txt\")):\n",
    "    train, temp = train_test_split(all_files, test_size=0.3, random_state=SEED)\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=SEED)\n",
    "    for name, data in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "        with open(f\"{split_dir}/{name}.txt\", \"w\") as f:\n",
    "            f.writelines(\"\\n\".join(data))\n",
    "    print(\"Created new train/val/test splits\")\n",
    "else:\n",
    "    print(\"Splits already exist — using existing split files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e811a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted masks: 701\n"
     ]
    }
   ],
   "source": [
    "# Stage 4 — Convert colored labels to single-channel class-ID masks (skip if done)\n",
    "color_to_id = {\n",
    "    \"(128,128,128)\": 0,\n",
    "    \"(128,0,0)\"    : 1,\n",
    "    \"(192,192,128)\": 2,\n",
    "    \"(128,64,128)\" : 3,\n",
    "    \"(0,0,192)\"    : 4,\n",
    "    \"(128,128,0)\"  : 5,\n",
    "    \"(192,128,128)\": 6,\n",
    "    \"(64,64,128)\"  : 7,\n",
    "    \"(64,0,128)\"   : 8,\n",
    "    \"(64,64,0)\"    : 9,\n",
    "    \"(0,128,192)\"  : 10\n",
    "}\n",
    "with open(f\"{CAMVID_ROOT}/mapping/color_to_classid.json\", \"w\") as f:\n",
    "    json.dump(color_to_id, f, indent=2)\n",
    "\n",
    "lookup = {eval(k): v for k, v in color_to_id.items()}\n",
    "\n",
    "lbl_src = f\"{CAMVID_ROOT}/labels\"\n",
    "lbl_out = f\"{CAMVID_ROOT}/labels_processed\"\n",
    "os.makedirs(lbl_out, exist_ok=True)\n",
    "\n",
    "# Process only if labels_processed empty\n",
    "if not os.listdir(lbl_out):\n",
    "    count = 0\n",
    "    for fname in sorted(os.listdir(lbl_src)):\n",
    "        if not fname.endswith(\".png\"):\n",
    "            continue\n",
    "        mask = np.array(Image.open(os.path.join(lbl_src, fname)).convert(\"RGB\"))\n",
    "        h,w,_ = mask.shape\n",
    "        new_mask = np.full((h,w), IGNORE_INDEX, dtype=np.uint8)\n",
    "        for rgb, cid in lookup.items():\n",
    "            new_mask[(mask == rgb).all(axis=2)] = cid\n",
    "        Image.fromarray(new_mask).save(os.path.join(lbl_out, fname))\n",
    "        count += 1\n",
    "    print(\"Converted masks:\", count)\n",
    "else:\n",
    "    print(\"Processed labels already present, skipping conversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "891a3fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and loaders ready (call make_loaders to create them).\n"
     ]
    }
   ],
   "source": [
    "# Stage 5 — Transforms and Dataset / DataLoader\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def train_transform(img, mask, size=IMG_SIZE):\n",
    "    if random.random() < 0.5:\n",
    "        img = TF.hflip(img)\n",
    "        mask = torch.flip(mask, dims=[1])\n",
    "    img = TF.resize(img, (size, size))\n",
    "    mask = TF.resize(mask.unsqueeze(0).float(), (size,size), interpolation=Image.NEAREST).squeeze(0).long()\n",
    "    img = TF.to_tensor(img)\n",
    "    img = TF.normalize(img, IMAGENET_MEAN, IMAGENET_STD)\n",
    "    return img, mask\n",
    "\n",
    "def val_transform(img, mask, size=IMG_SIZE):\n",
    "    img = TF.resize(img, (size, size))\n",
    "    mask = TF.resize(mask.unsqueeze(0).float(), (size,size), interpolation=Image.NEAREST).squeeze(0).long()\n",
    "    img = TF.to_tensor(img)\n",
    "    img = TF.normalize(img, IMAGENET_MEAN, IMAGENET_STD)\n",
    "    return img, mask\n",
    "\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, root, split_file, transform_fn=None):\n",
    "        self.root = Path(root)\n",
    "        self.img_dir = self.root / \"images\"\n",
    "        self.lbl_dir = self.root / \"labels_processed\"\n",
    "        self.transform_fn = transform_fn\n",
    "        with open(split_file, \"r\") as f:\n",
    "            self.items = [x.strip() for x in f.readlines()]\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.items[idx]\n",
    "        img = Image.open(self.img_dir / (name + \".png\")).convert(\"RGB\")\n",
    "        mask = Image.open(self.lbl_dir / (name + \".png\"))\n",
    "        mask = torch.from_numpy(np.array(mask)).long()\n",
    "        if self.transform_fn:\n",
    "            img, mask = self.transform_fn(img, mask)\n",
    "        return img, mask\n",
    "\n",
    "def make_loaders(root=CAMVID_ROOT, size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=0):\n",
    "    train_set = CamVidDataset(root, f\"{root}/splits/train.txt\", transform_fn=lambda i,m: train_transform(i,m,size))\n",
    "    val_set   = CamVidDataset(root, f\"{root}/splits/val.txt\", transform_fn=lambda i,m: val_transform(i,m,size))\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_set, batch_size=max(batch_size//2,1), shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "print(\"Dataset and loaders ready (call make_loaders to create them).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4d30e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast-SCNN sanity output shape: torch.Size([1, 11, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Stage 6 — Fast-SCNN model definition (same as existing)\n",
    "class DWConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, in_ch, 3, stride=stride, padding=1, groups=in_ch, bias=False),\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_ch, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class LinearBottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, t=6, stride=1):\n",
    "        super().__init__()\n",
    "        mid = in_ch * t\n",
    "        self.use_res = (stride == 1 and in_ch == out_ch)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, mid, 1, bias=False),\n",
    "            nn.BatchNorm2d(mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid, mid, 3, stride=stride, padding=1, groups=mid, bias=False),\n",
    "            nn.BatchNorm2d(mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        if self.use_res:\n",
    "            out = x + out\n",
    "        return out\n",
    "\n",
    "class GlobalFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_ch, block_channels, t=6):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for out_ch, stride in block_channels:\n",
    "            layers.append(LinearBottleneck(in_ch, out_ch, t=t, stride=stride))\n",
    "            in_ch = out_ch\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class FastSCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.learning_to_downsample = nn.Sequential(\n",
    "            DWConvBlock(3, 32, stride=2),\n",
    "            DWConvBlock(32, 48, stride=2),\n",
    "            DWConvBlock(48, 64, stride=2),\n",
    "        )\n",
    "        self.global_feature_extractor = GlobalFeatureExtractor(in_ch=64, block_channels=[(96,2),(128,1),(128,1)], t=6)\n",
    "        self.classifier = nn.Sequential(\n",
    "            DWConvBlock(128, 128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(128, NUM_CLASSES, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        x = self.learning_to_downsample(x)\n",
    "        x = self.global_feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, size=size, mode=\"bilinear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "# quick sanity check (optional)\n",
    "with torch.no_grad():\n",
    "    m = FastSCNN(num_classes=NUM_CLASSES)\n",
    "    o = m(torch.randn(1,3,256,256))\n",
    "    print(\"Fast-SCNN sanity output shape:\", o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008b9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 7 — Training utilities and train_model function\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, masks in tqdm(loader, desc=\"Train batches\"):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / max(1, len(loader))\n",
    "\n",
    "def evaluate_miou(model, loader, device, num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX):\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            ious = []\n",
    "            for cls in range(num_classes):\n",
    "                pred_c = (preds == cls)\n",
    "                mask_c = (masks == cls)\n",
    "                valid = (masks != ignore_index)\n",
    "                pred_c = pred_c & valid\n",
    "                mask_c = mask_c & valid\n",
    "                inter = (pred_c & mask_c).sum().item()\n",
    "                union = (pred_c | mask_c).sum().item()\n",
    "                if union > 0:\n",
    "                    ious.append(inter / union)\n",
    "            if len(ious) > 0:\n",
    "                total_iou += sum(ious) / len(ious)\n",
    "                count += 1\n",
    "    return total_iou / max(1, count)\n",
    "\n",
    "def train_model(root=CAMVID_ROOT, size=IMG_SIZE, num_classes=NUM_CLASSES, batch_size=BATCH_SIZE, lr=LR, epochs=EPOCHS, ignore_index=IGNORE_INDEX):\n",
    "    device = DEVICE\n",
    "    train_loader, val_loader = make_loaders(root, size, batch_size)\n",
    "    model = FastSCNN(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    best_miou = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_miou = evaluate_miou(model, val_loader, device, num_classes, ignore_index)\n",
    "        print(f\"Epoch: {epoch} Train Loss: {train_loss:.4f} Val mIoU: {val_miou:.4f}\")\n",
    "        if val_miou > best_miou:\n",
    "            best_miou = val_miou\n",
    "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, \"best_camvid.pth\"))\n",
    "            print(\"Saved best model\")\n",
    "        scheduler.step()\n",
    "    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, \"last_camvid.pth\"))\n",
    "    print(\"Training complete. Best mIoU:\", best_miou)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acef5c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.1312 Val mIoU: 0.2837\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.6675 Val mIoU: 0.3622\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.5457 Val mIoU: 0.3754\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.4846 Val mIoU: 0.3974\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.4456 Val mIoU: 0.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:24<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.4062 Val mIoU: 0.4063\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.3859 Val mIoU: 0.4328\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:27<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.3692 Val mIoU: 0.4222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.3515 Val mIoU: 0.4424\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:27<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.3340 Val mIoU: 0.4471\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train Loss: 0.3200 Val mIoU: 0.4596\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train Loss: 0.3092 Val mIoU: 0.4692\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train Loss: 0.2996 Val mIoU: 0.4673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train Loss: 0.2891 Val mIoU: 0.4730\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Train Loss: 0.2801 Val mIoU: 0.4678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Train Loss: 0.2706 Val mIoU: 0.4762\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:24<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Train Loss: 0.2717 Val mIoU: 0.4781\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Train Loss: 0.2586 Val mIoU: 0.4897\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:24<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Train Loss: 0.2596 Val mIoU: 0.4819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Train Loss: 0.2545 Val mIoU: 0.4794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Train Loss: 0.2447 Val mIoU: 0.4966\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Train Loss: 0.2343 Val mIoU: 0.4818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Train Loss: 0.2363 Val mIoU: 0.4924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Train Loss: 0.2300 Val mIoU: 0.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Train Loss: 0.2275 Val mIoU: 0.4997\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Train Loss: 0.2185 Val mIoU: 0.5049\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Train Loss: 0.2124 Val mIoU: 0.5090\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Train Loss: 0.2110 Val mIoU: 0.5066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Train Loss: 0.2085 Val mIoU: 0.5159\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Train Loss: 0.2114 Val mIoU: 0.5029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Train Loss: 0.1943 Val mIoU: 0.5171\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Train Loss: 0.1885 Val mIoU: 0.5179\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:27<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Train Loss: 0.1852 Val mIoU: 0.5181\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Train Loss: 0.1824 Val mIoU: 0.5187\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Train Loss: 0.1839 Val mIoU: 0.5197\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Train Loss: 0.1823 Val mIoU: 0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Train Loss: 0.1833 Val mIoU: 0.5135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Train Loss: 0.1826 Val mIoU: 0.5205\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Train Loss: 0.1794 Val mIoU: 0.5187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 Train Loss: 0.1792 Val mIoU: 0.5209\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 Train Loss: 0.1789 Val mIoU: 0.5184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:27<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 Train Loss: 0.1768 Val mIoU: 0.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:28<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 Train Loss: 0.1795 Val mIoU: 0.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:27<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 Train Loss: 0.1771 Val mIoU: 0.5205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 Train Loss: 0.1759 Val mIoU: 0.5193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 Train Loss: 0.1759 Val mIoU: 0.5178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 Train Loss: 0.1746 Val mIoU: 0.5168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:26<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 Train Loss: 0.1740 Val mIoU: 0.5199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 Train Loss: 0.1751 Val mIoU: 0.5207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 Train Loss: 0.1727 Val mIoU: 0.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:24<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 Train Loss: 0.1707 Val mIoU: 0.5210\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 Train Loss: 0.1707 Val mIoU: 0.5186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 Train Loss: 0.1726 Val mIoU: 0.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 Train Loss: 0.1707 Val mIoU: 0.5156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 Train Loss: 0.1710 Val mIoU: 0.5168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 Train Loss: 0.1687 Val mIoU: 0.5168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 Train Loss: 0.1721 Val mIoU: 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 Train Loss: 0.1687 Val mIoU: 0.5236\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 Train Loss: 0.1683 Val mIoU: 0.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|██████████| 123/123 [00:25<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 Train Loss: 0.1695 Val mIoU: 0.5165\n",
      "Training complete. Best mIoU: 0.5236152443604731\n"
     ]
    }
   ],
   "source": [
    "# Stage 8 — Start training (run this cell to train)\n",
    "# Note: training may take long on CPU. If you run on CPU, shorten epochs.\n",
    "trained_model = train_model(\n",
    "    root=CAMVID_ROOT,\n",
    "    size=IMG_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    epochs=EPOCHS,\n",
    "    ignore_index=IGNORE_INDEX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a17446a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mIoU: 0.49644736268824524\n"
     ]
    }
   ],
   "source": [
    "# Stage 9 — Evaluate on TEST set using saved best checkpoint\n",
    "def evaluate_test(weights=os.path.join(CHECKPOINT_DIR, \"best_camvid.pth\"), root=CAMVID_ROOT, size=IMG_SIZE):\n",
    "    device = DEVICE\n",
    "    test_set = CamVidDataset(root, f\"{root}/splits/test.txt\", transform_fn=lambda i,m: val_transform(i,m,size))\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "    model = FastSCNN(num_classes=NUM_CLASSES).to(device)\n",
    "    state = torch.load(weights, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    miou = evaluate_miou(model, test_loader, device, NUM_CLASSES, IGNORE_INDEX)\n",
    "    print(\"Test mIoU:\", miou)\n",
    "    return miou\n",
    "\n",
    "test_miou = evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f185d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparisons to outputs/comparisons and detections to outputs/detection\n"
     ]
    }
   ],
   "source": [
    "# Stage 10 — Visualization and detection extraction\n",
    "palette = {\n",
    "    0: (128,128,128), 1: (128,0,0), 2: (192,192,128),\n",
    "    3: (128,64,128),  4: (0,0,192), 5: (128,128,0),\n",
    "    6: (192,128,128), 7: (64,64,128), 8: (64,0,128),\n",
    "    9: (64,64,0),     10:(0,128,192)\n",
    "}\n",
    "\n",
    "def mask_to_color(mask):\n",
    "    h,w = mask.shape\n",
    "    out = np.zeros((h,w,3), dtype=np.uint8)\n",
    "    for cid, rgb in palette.items():\n",
    "        out[mask == cid] = rgb\n",
    "    return out\n",
    "\n",
    "def extract_bboxes_from_mask(mask, class_id):\n",
    "    \"\"\"\n",
    "    mask: 2D numpy array (H,W) predicted class ids\n",
    "    class_id: the class to extract boxes for\n",
    "    return: list of [x1,y1,x2,y2] in pixel coords\n",
    "    \"\"\"\n",
    "    binary = (mask == class_id).astype(np.uint8) * 255\n",
    "    if binary.sum() == 0:\n",
    "        return []\n",
    "    # find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = []\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        boxes.append([x, y, x+w, y+h])\n",
    "    return boxes\n",
    "\n",
    "def draw_boxes_on_image(img_rgb, boxes, color=(0,255,0), thickness=2, label=None):\n",
    "    # img_rgb: HxWx3 uint8\n",
    "    canvas = img_rgb.copy()\n",
    "    for i,box in enumerate(boxes):\n",
    "        x1,y1,x2,y2 = box\n",
    "        cv2.rectangle(canvas, (x1,y1), (x2,y2), color, thickness)\n",
    "        if label:\n",
    "            cv2.putText(canvas, label, (x1, max(y1-6,0)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    return canvas\n",
    "\n",
    "def visualize_and_save_all(\n",
    "    weights=os.path.join(CHECKPOINT_DIR, \"best_camvid.pth\"),\n",
    "    root=CAMVID_ROOT,\n",
    "    size=IMG_SIZE,\n",
    "    save_detection=True,\n",
    "    classes_to_box=[1,4,5]  # example: road? person? car? adjust to your mapping\n",
    "):\n",
    "    device = DEVICE\n",
    "    dataset = CamVidDataset(root, f\"{root}/splits/test.txt\", transform_fn=lambda i,m: val_transform(i,m,size))\n",
    "    model = FastSCNN(num_classes=NUM_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(weights, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        img_t, gt_mask = dataset[idx]\n",
    "        img_in = img_t.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(img_in)\n",
    "            pred = torch.argmax(out, dim=1).squeeze().cpu().numpy()\n",
    "        gt = gt_mask.numpy()\n",
    "        # convert input tensor back to displayable RGB (unnormalize)\n",
    "        img_np = img_t.cpu().numpy().transpose(1,2,0)\n",
    "        img_np = (img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN))\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        img_disp = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "        gt_rgb = mask_to_color(gt)\n",
    "        pred_rgb = mask_to_color(pred)\n",
    "\n",
    "        # save comparison side by side\n",
    "        comp = np.concatenate([img_disp, gt_rgb, pred_rgb], axis=1)\n",
    "        comp_pil = Image.fromarray(comp)\n",
    "        comp_pil.save(os.path.join(OUTPUT_COMP, f\"comp_{idx}.png\"))\n",
    "\n",
    "        # detection overlay: draw boxes for selected classes\n",
    "        if save_detection:\n",
    "            det_img = img_disp.copy()\n",
    "            for cls in classes_to_box:\n",
    "                boxes = extract_bboxes_from_mask(pred, cls)\n",
    "                label = f\"class_{cls}\"\n",
    "                # color pick\n",
    "                col = (0,255,0) if cls==5 else (255,0,0)  # example: class 5 as green (car), else red\n",
    "                det_img = draw_boxes_on_image(det_img, boxes, color=col, thickness=2, label=label)\n",
    "            Image.fromarray(det_img).save(os.path.join(OUTPUT_DET, f\"det_{idx}.png\"))\n",
    "\n",
    "    print(\"Saved comparisons to\", OUTPUT_COMP, \"and detections to\", OUTPUT_DET)\n",
    "\n",
    "# Run visualization & detection extraction for all test images (or limit by slicing)\n",
    "visualize_and_save_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
