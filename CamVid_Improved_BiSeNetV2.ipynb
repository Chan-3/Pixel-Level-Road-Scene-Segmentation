{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d003f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu Image size: 360 x 480\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CELL 1 - Imports and global configuration\n",
    "# ------------------------------\n",
    "import os, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "# Settings (change if needed)\n",
    "DATA_ROOT = \"camvid\"           # expects camvid/images, camvid/labels_processed, camvid/splits/*.txt\n",
    "IMG_H, IMG_W = 360, 480\n",
    "NUM_CLASSES = 11\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 30\n",
    "LR = 5e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 0                # set 0 in notebook/Windows\n",
    "USE_PRETRAINED_BACKBONE = True\n",
    "\n",
    "# outputs\n",
    "os.makedirs(\"outputs/bisenet_checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"outputs/bisenet_predictions\", exist_ok=True)\n",
    "os.makedirs(\"outputs/bisenet_comparisons\", exist_ok=True)\n",
    "\n",
    "# seeds\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "print(\"Device:\", DEVICE, \"Image size:\", IMG_H, \"x\", IMG_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1251f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CELL 2 - Color <-> class utilities\n",
    "# ------------------------------\n",
    "color_to_id = {\n",
    "    (128,128,128): 0,\n",
    "    (128,0,0): 1,\n",
    "    (192,192,128): 2,\n",
    "    (128,64,128): 3,\n",
    "    (0,0,192): 4,\n",
    "    (128,128,0): 5,\n",
    "    (192,128,128): 6,\n",
    "    (64,64,128): 7,\n",
    "    (64,0,128): 8,\n",
    "    (64,64,0): 9,\n",
    "    (0,128,192): 10\n",
    "}\n",
    "id_to_color = {v:k for k,v in color_to_id.items()}\n",
    "\n",
    "def class_mask_to_rgb(mask):\n",
    "    # mask: H x W integer class ids (0..10, 255)\n",
    "    h,w = mask.shape\n",
    "    rgb = np.zeros((h,w,3), dtype=np.uint8)\n",
    "    for cid,color in id_to_color.items():\n",
    "        rgb[mask==cid] = color\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb365e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CELL 3 - Transforms (train/val)\n",
    "# ------------------------------\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def random_scale_and_crop(img, mask, th, tw, min_s=0.95, max_s=1.05):\n",
    "    s = random.uniform(min_s, max_s)\n",
    "    nw, nh = max(1,int(img.width*s)), max(1,int(img.height*s))\n",
    "    img = img.resize((nw, nh), Image.BILINEAR)\n",
    "    mask = mask.resize((nw, nh), Image.NEAREST)\n",
    "    pad_h = max(0, th - nh); pad_w = max(0, tw - nw)\n",
    "    if pad_h or pad_w:\n",
    "        img = Image.fromarray(np.pad(np.array(img), ((0,pad_h),(0,pad_w),(0,0)), mode='reflect'))\n",
    "        mask = Image.fromarray(np.pad(np.array(mask), ((0,pad_h),(0,pad_w)), mode='constant', constant_values=255))\n",
    "    left = random.randint(0, max(0, nw - tw)); top = random.randint(0, max(0, nh - th))\n",
    "    img = img.crop((left, top, left+tw, top+th))\n",
    "    mask = mask.crop((left, top, left+tw, top+th))\n",
    "    return img, mask\n",
    "\n",
    "def train_transform(img, mask, h=IMG_H, w=IMG_W):\n",
    "    # flip + small scale crop + normalize\n",
    "    if random.random() < 0.5:\n",
    "        img = TF.hflip(img)\n",
    "        mask = Image.fromarray(np.flip(np.array(mask), axis=1))\n",
    "    img, mask = random_scale_and_crop(img, mask, h, w)\n",
    "    img = TF.to_tensor(img); img = TF.normalize(img, IMAGENET_MEAN, IMAGENET_STD)\n",
    "    mask = torch.from_numpy(np.array(mask)).long()\n",
    "    return img, mask\n",
    "\n",
    "def val_transform(img, mask, h=IMG_H, w=IMG_W):\n",
    "    img = img.resize((w,h), Image.BILINEAR); mask = mask.resize((w,h), Image.NEAREST)\n",
    "    img = TF.to_tensor(img); img = TF.normalize(img, IMAGENET_MEAN, IMAGENET_STD)\n",
    "    mask = torch.from_numpy(np.array(mask)).long()\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70851c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 490 Val: 105 Test: 106\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CELL 4 - Dataset and DataLoaders\n",
    "# ------------------------------\n",
    "class CamVidDataset(Dataset):\n",
    "    def __init__(self, root, split_file, mode=\"val\"):\n",
    "        self.root = Path(root)\n",
    "        self.img_dir = self.root / \"images\"\n",
    "        self.lbl_dir = self.root / \"labels_processed\"\n",
    "        with open(split_file, \"r\") as f:\n",
    "            self.items = [x.strip() for x in f.readlines()]\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.items[idx]\n",
    "        img = Image.open(self.img_dir/(name+\".png\")).convert(\"RGB\")\n",
    "        mask = Image.open(self.lbl_dir/(name+\".png\"))\n",
    "        return (train_transform(img, mask) if self.mode==\"train\" else val_transform(img, mask))\n",
    "\n",
    "def make_loaders(root=DATA_ROOT, batch_size=BATCH_SIZE):\n",
    "    train_set = CamVidDataset(root, f\"{root}/splits/train.txt\", mode=\"train\")\n",
    "    val_set   = CamVidDataset(root, f\"{root}/splits/val.txt\", mode=\"val\")\n",
    "    test_set  = CamVidDataset(root, f\"{root}/splits/test.txt\", mode=\"val\")\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders()\n",
    "print(\"Train:\", len(train_loader.dataset), \"Val:\", len(val_loader.dataset), \"Test:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7b6bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model OK. Params: 4 M\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CELL 5 - BiSeNetV2-Lite model (FINAL FIXED VERSION)\n",
    "# ------------------------------\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "class DetailBranch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.s2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SemanticBranch(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        mb = mobilenet_v2(weights=weights)\n",
    "        features = mb.features\n",
    "\n",
    "        # stages\n",
    "        self.stage1 = nn.Sequential(*features[:4])   # output channels: 24\n",
    "        self.stage2 = nn.Sequential(*features[4:7])  # output channels: 32\n",
    "        self.stage3 = nn.Sequential(*features[7:])   # output channels: 1280 (IMPORTANT)\n",
    "\n",
    "        # FIXED REDUCTION LAYERS\n",
    "        self.reduce_low  = nn.Conv2d(24,   64, 1, bias=False)\n",
    "        self.reduce_mid  = nn.Conv2d(32,  128, 1, bias=False)\n",
    "        self.reduce_high = nn.Conv2d(1280, 128, 1, bias=False)   \n",
    "\n",
    "        self.bn   = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1 = self.stage1(x)\n",
    "        s2 = self.stage2(s1)\n",
    "        s3 = self.stage3(s2)\n",
    "\n",
    "        r1 = self.reduce_low(s1)\n",
    "        r2 = self.reduce_mid(s2)\n",
    "        r3 = self.reduce_high(s3)\n",
    "\n",
    "        # upsample and fuse\n",
    "        r3u = nn.functional.interpolate(r3, size=r2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        fuse = self.bn(self.relu(r2 + r3u))\n",
    "\n",
    "        return r1, fuse\n",
    "\n",
    "\n",
    "class BiSeNetV2Lite(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, pretrained_backbone=True):\n",
    "        super().__init__()\n",
    "        self.detail = DetailBranch()\n",
    "        self.semantic = SemanticBranch(pretrained=pretrained_backbone)\n",
    "\n",
    "        # detail produces 128 channels\n",
    "        # semantic produces 128 channels\n",
    "        # so combined = 256\n",
    "        self.fuse_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d = self.detail(x)\n",
    "        low, sem = self.semantic(x)\n",
    "\n",
    "        sem_up = nn.functional.interpolate(sem, size=d.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        cat = torch.cat([d, sem_up], dim=1)\n",
    "        fused = self.fuse_conv(cat)\n",
    "        out = self.classifier(fused)\n",
    "\n",
    "        # final upsample to input size\n",
    "        return nn.functional.interpolate(out, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = BiSeNetV2Lite()\n",
    "print(\"Model OK. Params:\", sum(p.numel() for p in model.parameters())//1_000_000, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f98cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CELL 6 - Loss, optimizer, LR schedule, mIoU\n",
    "# ------------------------------\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "def poly_lr(optimizer, init_lr, cur_iter, max_iter, power=0.9):\n",
    "    lr = init_lr * (1 - float(cur_iter)/float(max_iter))**power\n",
    "    for g in optimizer.param_groups: g['lr'] = lr\n",
    "\n",
    "def compute_miou_batch(preds, masks, num_classes=NUM_CLASSES, ignore_index=255):\n",
    "    preds = preds.cpu().numpy(); masks = masks.cpu().numpy()\n",
    "    total_iou = 0.0; cnt = 0\n",
    "    for p,m in zip(preds,masks):\n",
    "        valid = (m != ignore_index); ious=[]\n",
    "        for cls in range(num_classes):\n",
    "            pred_c = (p==cls); mask_c = (m==cls)\n",
    "            inter = (pred_c & mask_c & valid).sum()\n",
    "            union = ((pred_c | mask_c) & valid).sum()\n",
    "            if union>0: ious.append(inter/union)\n",
    "        if ious: total_iou += sum(ious)/len(ious); cnt+=1\n",
    "    return total_iou / max(1,cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "204a2e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 loss=0.7829 val_mIoU=0.4616 time=143.8s lr=4.85e-04\n",
      "  Saved best model.\n",
      "Epoch 2/30 loss=0.4780 val_mIoU=0.5083 time=145.9s lr=4.70e-04\n",
      "  Saved best model.\n",
      "Epoch 3/30 loss=0.3969 val_mIoU=0.5018 time=136.6s lr=4.55e-04\n",
      "Epoch 4/30 loss=0.3346 val_mIoU=0.5275 time=134.4s lr=4.40e-04\n",
      "  Saved best model.\n",
      "Epoch 5/30 loss=0.3032 val_mIoU=0.5169 time=135.7s lr=4.24e-04\n",
      "Epoch 6/30 loss=0.2879 val_mIoU=0.4832 time=136.1s lr=4.09e-04\n",
      "Epoch 7/30 loss=0.2696 val_mIoU=0.5343 time=132.2s lr=3.94e-04\n",
      "  Saved best model.\n",
      "Epoch 8/30 loss=0.2553 val_mIoU=0.5440 time=134.1s lr=3.78e-04\n",
      "  Saved best model.\n",
      "Epoch 9/30 loss=0.2484 val_mIoU=0.5525 time=136.1s lr=3.63e-04\n",
      "  Saved best model.\n",
      "Epoch 10/30 loss=0.2212 val_mIoU=0.5671 time=134.9s lr=3.47e-04\n",
      "  Saved best model.\n",
      "Epoch 11/30 loss=0.2323 val_mIoU=0.5592 time=134.6s lr=3.31e-04\n",
      "Epoch 12/30 loss=0.2075 val_mIoU=0.5451 time=131.6s lr=3.16e-04\n",
      "Epoch 13/30 loss=0.1923 val_mIoU=0.5825 time=131.9s lr=3.00e-04\n",
      "  Saved best model.\n",
      "Epoch 14/30 loss=0.2101 val_mIoU=0.5634 time=133.5s lr=2.84e-04\n",
      "Epoch 15/30 loss=0.2014 val_mIoU=0.5668 time=135.1s lr=2.68e-04\n",
      "Epoch 16/30 loss=0.2018 val_mIoU=0.5846 time=133.3s lr=2.52e-04\n",
      "  Saved best model.\n",
      "Epoch 17/30 loss=0.1725 val_mIoU=0.5724 time=134.0s lr=2.36e-04\n",
      "Epoch 18/30 loss=0.1716 val_mIoU=0.5907 time=135.4s lr=2.19e-04\n",
      "  Saved best model.\n",
      "Epoch 19/30 loss=0.1680 val_mIoU=0.5920 time=136.9s lr=2.03e-04\n",
      "  Saved best model.\n",
      "Epoch 20/30 loss=0.1738 val_mIoU=0.5958 time=138.1s lr=1.86e-04\n",
      "  Saved best model.\n",
      "Epoch 21/30 loss=0.1586 val_mIoU=0.5959 time=134.3s lr=1.69e-04\n",
      "Epoch 22/30 loss=0.1698 val_mIoU=0.6078 time=134.5s lr=1.52e-04\n",
      "  Saved best model.\n",
      "Epoch 23/30 loss=0.1500 val_mIoU=0.6083 time=133.5s lr=1.35e-04\n",
      "  Saved best model.\n",
      "Epoch 24/30 loss=0.1570 val_mIoU=0.6044 time=133.2s lr=1.17e-04\n",
      "Epoch 25/30 loss=0.1533 val_mIoU=0.6057 time=133.7s lr=9.97e-05\n",
      "Epoch 26/30 loss=0.1518 val_mIoU=0.6077 time=133.2s lr=8.15e-05\n",
      "Epoch 27/30 loss=0.1552 val_mIoU=0.6139 time=133.8s lr=6.29e-05\n",
      "  Saved best model.\n",
      "Epoch 28/30 loss=0.1416 val_mIoU=0.6085 time=134.9s lr=4.37e-05\n",
      "Epoch 29/30 loss=0.1470 val_mIoU=0.6165 time=137.0s lr=2.34e-05\n",
      "  Saved best model.\n",
      "Epoch 30/30 loss=0.1418 val_mIoU=0.6209 time=137.6s lr=0.00e+00\n",
      "  Saved best model.\n",
      "Training finished. Best mIoU: 0.620941111283765\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CELL 7 - Training loop (poly LR + early stopping)\n",
    "# ------------------------------\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "def train_bisenet(epochs=EPOCHS, init_lr=LR, patience=8):\n",
    "    best_miou = -1.0; no_improve = 0\n",
    "    max_iter = epochs * len(train_loader); iter_count = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train(); running_loss=0.0; t0=time.time()\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs = imgs.to(DEVICE); masks = masks.to(DEVICE)\n",
    "            iter_count += 1; poly_lr(optimizer, init_lr, iter_count, max_iter)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, masks)\n",
    "            loss.backward(); optimizer.step()\n",
    "            running_loss += float(loss.item())\n",
    "\n",
    "        # validation\n",
    "        model.eval(); val_iou_sum=0.0; nval=0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs = imgs.to(DEVICE); masks = masks.to(DEVICE)\n",
    "                out = model(imgs); preds = torch.argmax(out, dim=1)\n",
    "                val_iou_sum += compute_miou_batch(preds, masks); nval += 1\n",
    "\n",
    "        val_miou = val_iou_sum / max(1, nval)\n",
    "        print(f\"Epoch {epoch}/{epochs} loss={running_loss/len(train_loader):.4f} val_mIoU={val_miou:.4f} time={time.time()-t0:.1f}s lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # checkpoint\n",
    "        if val_miou > best_miou + 1e-4:\n",
    "            best_miou = val_miou\n",
    "            torch.save(model.state_dict(), \"outputs/bisenet_checkpoints/best_bisenet.pth\")\n",
    "            print(\"  Saved best model.\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping at epoch\", epoch)\n",
    "                break\n",
    "\n",
    "    print(\"Training finished. Best mIoU:\", best_miou)\n",
    "\n",
    "# run training\n",
    "train_bisenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a1689d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiSeNetV2 Test mIoU: 0.6197696506028855\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_bisenet(\n",
    "    weights=\"outputs/bisenet_checkpoints/best_bisenet.pth\",\n",
    "    root=DATA_ROOT,\n",
    "    h=IMG_H,\n",
    "    w=IMG_W\n",
    "):\n",
    "    device = DEVICE\n",
    "\n",
    "    # Test dataset & loader\n",
    "    test_set = CamVidDataset(\n",
    "        root,\n",
    "        f\"{root}/splits/test.txt\",\n",
    "        mode=\"val\"   # val_transform used internally\n",
    "    )\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Load model\n",
    "    model = BiSeNetV2Lite(num_classes=NUM_CLASSES).to(device)\n",
    "    state = torch.load(weights, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    # Compute mIoU\n",
    "    miou = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            out = model(imgs)\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "            miou += compute_miou_batch(preds, masks)\n",
    "            count += 1\n",
    "\n",
    "    miou = miou / max(1, count)\n",
    "    print(\"BiSeNetV2 Test mIoU:\", miou)\n",
    "    return miou\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "test_miou_bisenet = evaluate_test_bisenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d028c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best BiSeNet model.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CELL 8 - Load best checkpoint and prepare for inference\n",
    "# ------------------------------\n",
    "best_model = BiSeNetV2Lite()\n",
    "best_model.load_state_dict(torch.load(\"outputs/bisenet_checkpoints/best_bisenet.pth\", map_location=DEVICE))\n",
    "best_model = best_model.to(DEVICE); best_model.eval()\n",
    "print(\"Loaded best BiSeNet model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4bc0e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions and comparisons.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CELL 9 - Save predictions and side-by-side comparisons for all test images\n",
    "# ------------------------------\n",
    "def save_all_predictions_and_comparisons(out_pred_dir=\"outputs/bisenet_predictions\", out_comp_dir=\"outputs/bisenet_comparisons\"):\n",
    "    os.makedirs(out_pred_dir, exist_ok=True); os.makedirs(out_comp_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for idx, (imgs, masks) in enumerate(test_loader):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            masks_np = masks.numpy()[0]\n",
    "            out = best_model(imgs)                      # (B,C,H,W)\n",
    "            pred = torch.argmax(out, dim=1).cpu().numpy()[0]\n",
    "            img_np = imgs.cpu().numpy()[0].transpose(1,2,0)\n",
    "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "            gt_rgb = class_mask_to_rgb(masks_np); pred_rgb = class_mask_to_rgb(pred)\n",
    "            Image.fromarray(pred_rgb).save(f\"{out_pred_dir}/pred_{idx}.png\")\n",
    "            fig, ax = plt.subplots(1,3,figsize=(14,4))\n",
    "            ax[0].imshow(img_np); ax[0].set_title(\"Image\"); ax[0].axis(\"off\")\n",
    "            ax[1].imshow(gt_rgb);  ax[1].set_title(\"GT Mask\"); ax[1].axis(\"off\")\n",
    "            ax[2].imshow(pred_rgb);ax[2].set_title(\"Pred Mask\"); ax[2].axis(\"off\")\n",
    "            fig.tight_layout(); fig.savefig(f\"{out_comp_dir}/comparison_{idx}.png\", dpi=120); plt.close(fig)\n",
    "    print(\"Saved predictions and comparisons.\")\n",
    "\n",
    "save_all_predictions_and_comparisons()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
